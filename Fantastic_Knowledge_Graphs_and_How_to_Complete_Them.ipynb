{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nf7SvDYeKzb"
      },
      "source": [
        "# Required Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUH_fyiTCsWK",
        "outputId": "a619123f-7a32-4b69-e4c2-90b606ac0b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ogb in /usr/local/lib/python3.7/dist-packages (1.3.2)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: outdated>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (0.2.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.1.5)\n",
            "Requirement already satisfied: littleutils in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "1.10.0+cu111\n",
            "11.1\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.7/dist-packages (2.0.9)\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.7/dist-packages (0.6.12)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.19.5)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.7/dist-packages (2.0.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (6.0.2)\n",
            "Requirement already satisfied: yacs in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.1.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.1.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.62.3)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.19.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from rdflib->torch-geometric) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2021.10.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "# may take around 5-10 minutes\n",
        "!pip install ogb\n",
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "!python -c \"import torch; print(torch.version.cuda)\"\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu111.html\n",
        "!pip install torch-geometric\n",
        "!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
        "\n",
        "import numpy as np\n",
        "import ogb\n",
        "import os\n",
        "import pdb\n",
        "import random\n",
        "import torch\n",
        "import torch_geometric\n",
        "import tqdm\n",
        "from ogb.linkproppred import LinkPropPredDataset, PygLinkPropPredDataset\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGqNi004hHBo"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFZnTwVShkK5"
      },
      "source": [
        "We focus here on a collaboratively collected knowledge base known by the name of **FB15K-237**, which is derived from Freebase repository. FB15K-237 is a cleaner version of its original counterpart FB15k with inverse relations removed and consists of 14541 entities and 237 relations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjYd5Z7wtSbp",
        "outputId": "56e1ad53-013d-44d5-f281-191af20aa4c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2021-12-10 04:20:34--  https://github.com/kanishkg/fb15k-237/raw/main/fb15k-237-v1.zip\n",
            "Resolving github.com (github.com)... 52.69.186.44\n",
            "Connecting to github.com (github.com)|52.69.186.44|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/kanishkg/fb15k-237/main/fb15k-237-v1.zip [following]\n",
            "--2021-12-10 04:20:35--  https://raw.githubusercontent.com/kanishkg/fb15k-237/main/fb15k-237-v1.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9452681 (9.0M) [application/zip]\n",
            "Saving to: ‘fb15k-237-v1.zip’\n",
            "\n",
            "fb15k-237-v1.zip    100%[===================>]   9.01M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-12-10 04:20:36 (80.8 MB/s) - ‘fb15k-237-v1.zip’ saved [9452681/9452681]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget \"https://github.com/kanishkg/fb15k-237/raw/main/fb15k-237-v1.zip\"\n",
        "!tar -xzf fb15k-237-v1.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42ieIb_XFKHC"
      },
      "outputs": [],
      "source": [
        "meta_dict = {'eval metric': 'mrr',\n",
        "             'task type': 'KG_completion',\n",
        "             'download_name': 'fb15k-237',\n",
        "             'version': '1',\n",
        "             'url': 'https://drive.google.com/file/d/1NoebmUcqHheuWCyye6e3zm_rsbM6gvyj/view?usp=sharing',\n",
        "             'add_inverse_edge': False,\n",
        "             'has_node_attr': False,\n",
        "             'has_edge_attr': False,\n",
        "             'split': 'msr',\n",
        "             'additional node files': 'None',\n",
        "             'additional edge files': 'edge_reltype',\n",
        "             'is hetero': False,\n",
        "             'binary': False,\n",
        "             'dir_path': './fb15k-237-v1',\n",
        "             }\n",
        "fb_dataset = PygLinkPropPredDataset(name='ogb-fb15k237', meta_dict=meta_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aT45DA5ZFnb"
      },
      "outputs": [],
      "source": [
        "# split the dataset using the ogb function\n",
        "split_edge = fb_dataset.get_edge_split()\n",
        "train_edge, valid_edge, test_edge = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHeEKSGwgTNb"
      },
      "outputs": [],
      "source": [
        "true_edges = torch.load('./fb15k-237-v1/processed/data_processed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WRbFS7mlnSa",
        "outputId": "83132232-c6c1-4432-ae15-bbcbf72e20da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'edge_feat': None,\n",
              " 'edge_index': tensor([[ 3818,   819,  9791,  ..., 11737, 12535,  3203],\n",
              "         [ 8942,  9234,   756,  ...,  4042,  9678, 10433]]),\n",
              " 'edge_reltype': tensor([[122],\n",
              "         [230],\n",
              "         [140],\n",
              "         ...,\n",
              "         [198],\n",
              "         [ 13],\n",
              "         [ 11]]),\n",
              " 'node_feat': None,\n",
              " 'num_nodes': 14541}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTubr13J_lg"
      },
      "source": [
        "\n",
        "\n",
        "# Relation Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2-XLS-ihnsq"
      },
      "source": [
        "We define our dataset class here that generates both positive and negative  triples for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZoVWretKP0r"
      },
      "outputs": [],
      "source": [
        "class RelationDataset(Dataset):\n",
        "  def __init__(self, edges, true_edges, filter=False):\n",
        "    self.true_edges = true_edges\n",
        "    self.train_edges = edges\n",
        "    self.edge_index = edges['edge_index']\n",
        "    self.edge_reltype = edges['edge_reltype']\n",
        "    self.num_nodes = edges['num_nodes']\n",
        "    self.num_rels = 237\n",
        "    self.rel_dict = {}\n",
        "    self.true_edge_dict = {}\n",
        "    self.filter = filter\n",
        "\n",
        "    # We construct a dictionary that maps edges to relation types\n",
        "    # We do this to quickly filter out postive edges while sampling negative\n",
        "    # edges.\n",
        "    for i in range(self.true_edges['edge_index'].shape[1]):\n",
        "      h = self.true_edges['edge_index'][0, i]\n",
        "      t = self.true_edges['edge_index'][1, i]\n",
        "      r = self.true_edges['edge_reltype'][i, 0]\n",
        "      if (h,t) not in self.true_edge_dict:\n",
        "        self.true_edge_dict[(h,t)] = []\n",
        "      self.true_edge_dict[(h,t)].append(r)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.edge_index.size(1)\n",
        "\n",
        "  def _sample_negative_edge(self, idx):\n",
        "    sample = random.uniform(0, 1)\n",
        "    found = False\n",
        "    while not found:\n",
        "      if sample <= 0.4:\n",
        "        # corrupt the head entity\n",
        "        h = self.edge_index[0, idx]\n",
        "        t = torch.randint(0, self.num_nodes, (1,))\n",
        "        r = self.edge_reltype[idx,:]\n",
        "      elif 0.4 < sample < 0.8:\n",
        "        # corrupt the tail entity\n",
        "        t = self.edge_index[1, idx]\n",
        "        h = torch.randint(0, self.num_nodes, (1,))\n",
        "        r = self.edge_reltype[idx,:]\n",
        "      else:\n",
        "        # corrupt the relation\n",
        "        # adding this auxilliary loss is shown to improve performance\n",
        "        t = self.edge_index[1, idx]\n",
        "        h = self.edge_index[0, idx]\n",
        "        r = torch.randint(0, self.num_rels, (1,))\n",
        "      if not self.filter:\n",
        "        found = True\n",
        "      else:\n",
        "        # check if the edge is a true edge\n",
        "        if (h, t) not in self.true_edge_dict:\n",
        "          found = True\n",
        "        elif r not in self.true_edge_dict[(h, t)]:\n",
        "          found = True\n",
        "    data = [torch.tensor([h,t]), r]\n",
        "    return data\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pos_sample = [self.edge_index[:, idx], self.edge_reltype[idx,:]]\n",
        "    neg_sample = self._sample_negative_edge(idx)\n",
        "    return pos_sample, neg_sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI4-LGMBTXtz"
      },
      "outputs": [],
      "source": [
        "class TestRelationDataset(Dataset):\n",
        "  def __init__(self, edges, true_edges, filter=False, num_neg=14000, mode='head'):\n",
        "    self.true_edges = true_edges\n",
        "    self.edge_index = edges['edge_index']\n",
        "    self.edge_reltype = edges['edge_reltype']\n",
        "    self.num_nodes = edges['num_nodes']\n",
        "    self.num_neg = num_neg\n",
        "    self.mode = mode\n",
        "    self.true_edge_dict = {}\n",
        "    self.filter = filter\n",
        "\n",
        "    # We construct a dictionary that maps edges to relation types\n",
        "    # We do this to quickly filter out postive edges while sampling negative\n",
        "    # edges.\n",
        "    for i in range(self.true_edges['edge_index'].shape[1]):\n",
        "      h = self.true_edges['edge_index'][0, i]\n",
        "      t = self.true_edges['edge_index'][1, i]\n",
        "      r = self.true_edges['edge_reltype'][i, 0]\n",
        "      if (h,t) not in self.true_edge_dict:\n",
        "        self.true_edge_dict[(h,t)] = []\n",
        "      self.true_edge_dict[(h,t)].append(r)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.edge_index.size(1)\n",
        "\n",
        "  def _sample_negative_edge(self, idx, mode):\n",
        "    triples = []\n",
        "    random_node_idx = list(range(self.num_nodes))\n",
        "    random.shuffle(random_node_idx)\n",
        "    for n in random_node_idx:\n",
        "      r = self.edge_reltype[idx,:]\n",
        "      if mode == 'head':\n",
        "        # corrupt tail if in head mode\n",
        "        t = torch.tensor(n)\n",
        "        h = self.edge_index[0, idx]\n",
        "      elif mode == 'tail':\n",
        "        # corrupt head if in tail mode\n",
        "        h = torch.tensor(n)\n",
        "        t = self.edge_index[1, idx]\n",
        "      ht = torch.tensor([h, t])\n",
        "      if self.filter:\n",
        "        # check if edge is present in the knowledge graph\n",
        "        if (h, t) not in self.true_edge_dict:\n",
        "          triples.append([ht, r])\n",
        "        elif r not in self.true_edge_dict[(h, t)]:\n",
        "            triples.append([ht, r])\n",
        "      else:\n",
        "          triples.append([ht, r])\n",
        "      if len(triples) == self.num_neg:\n",
        "        break\n",
        "\n",
        "    return triples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    pos_sample = [self.edge_index[:, idx], self.edge_reltype[idx,:]]\n",
        "    neg_samples = self._sample_negative_edge(idx, mode=self.mode)\n",
        "    edges = torch.stack([pos_sample[0]] + [ht for ht, _ in neg_samples])\n",
        "    edge_reltype = torch.stack([pos_sample[1]] + [r for _, r in neg_samples])\n",
        "    return edges, edge_reltype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYiUGJVIS-XW"
      },
      "source": [
        "# Knowledge Graph Models and their Loss Functions\n",
        "\n",
        "We define our model classes and there respective loss funtions here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvkvdbYVsEmW"
      },
      "source": [
        "**TransE**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "TransE is based on the simple idea that the entities and relations can be seen as embeddings in a vector space such that head entity embedding and relation embedding can be added to give tail entity emebdding.\n",
        "\n",
        "The scoring function for a positive example <h, r, t> is defined as negative of the distance, or mathematically - || h + r - t || so that distance is as low as possible for positive examples. Loss function can then be defined as a max-margin loss which maximizes the distance for negative examples and minimizes for postive examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Lc40iEvsMO-"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TransE(nn.Module):\n",
        "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
        "        super(TransE, self).__init__()\n",
        "        self.entity_embeddings = torch.nn.Parameter(torch.randn(num_entities, embedding_dim))\n",
        "        self.relation_embeddings = torch.nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
        "\n",
        "    def forward(self):\n",
        "        self.entity_embeddings.data[:-1, :].div_(\n",
        "            self.entity_embeddings.data[:-1, :].norm(p=2, dim=1, keepdim=True))\n",
        "        return self.entity_embeddings, self.relation_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BpJmEB0TOba"
      },
      "source": [
        "TransE Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i3du7qbTMGx"
      },
      "outputs": [],
      "source": [
        "def TransE_loss(pos_edges, neg_edges, pos_reltype, neg_reltype, entity_embeddings,\n",
        "                relation_embeddings):\n",
        "  # Select embeddings for both positive and negative samples\n",
        "  pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
        "  pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
        "  neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
        "  neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
        "  pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
        "  neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
        "\n",
        "  # Calculate the distance score\n",
        "  d_pos = torch.norm(pos_head_embeds + pos_relation_embeds - pos_tail_embeds, p=1, dim=1)\n",
        "  d_neg = torch.norm(neg_head_embeds + neg_relation_embeds - neg_tail_embeds, p=1, dim=1)\n",
        "  ones = torch.ones(d_pos.size(0))\n",
        "\n",
        "  # margin loss - we want to increase d_neg and decrease d_pos\n",
        "  margin_loss = torch.nn.MarginRankingLoss(margin=1.)\n",
        "  loss = margin_loss(d_neg, d_pos, ones)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QYe-nez_l2V"
      },
      "source": [
        "**ComplEx**\n",
        "\n",
        "\n",
        "---\n",
        "ComplEx model proposes that we represent the entity and triple embeddings in a complex vector space. In ComplEx, we learn embeddings by treating the problem as a binary classification problem where the goal is to classify each triple as either positive (0) or corrupt (1).  \n",
        "\n",
        "For a triple <h, r, t>, the similarity function takes the dot product of h, r and the complex conjugate of t and returns the real value of the product. Intuitively, this measures the similarity (specifically cosine similarity) between <h, r> and the complex conjugate of t.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nfvzMb6_lCy"
      },
      "outputs": [],
      "source": [
        "class ComplEx(nn.Module):\n",
        "  def __init__(self, num_entities, num_relations, embedding_dim):\n",
        "    super(ComplEx, self).__init__()\n",
        "    self.entity_embeddings = torch.nn.Parameter(torch.randn(num_entities, embedding_dim))\n",
        "    self.relation_embeddings = torch.nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
        "\n",
        "  def forward(self):\n",
        "    # return the embeddings as it is but we can regularize here by normalizing them\n",
        "    return self.entity_embeddings, self.relation_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-q6tY1uTQBO"
      },
      "source": [
        "ComplEx Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STvQi6pCCAm5"
      },
      "outputs": [],
      "source": [
        "def ComplEx_loss(pos_edges, neg_edges, pos_reltype, neg_reltype,\n",
        "                 entity_embeddings, relation_embeddings, reg=1e-3):\n",
        "  # Select embeddings for both positive and negative samples\n",
        "  pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
        "  pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
        "  neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
        "  neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
        "  pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
        "  neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
        "\n",
        "  # Get real and imaginary parts\n",
        "  pos_re_relation, pos_im_relation = torch.chunk(pos_relation_embeds, 2, dim=1)\n",
        "  neg_re_relation, neg_im_relation = torch.chunk(neg_relation_embeds, 2, dim=1)\n",
        "  pos_re_head, pos_im_head = torch.chunk(pos_head_embeds, 2, dim=1)\n",
        "  pos_re_tail, pos_im_tail = torch.chunk(pos_tail_embeds, 2, dim=1)\n",
        "  neg_re_head, neg_im_head = torch.chunk(neg_head_embeds, 2, dim=1)\n",
        "  neg_re_tail, neg_im_tail = torch.chunk(neg_tail_embeds, 2, dim=1)\n",
        "\n",
        "  # Compute pos score\n",
        "  pos_re_score = pos_re_head * pos_re_relation - pos_im_head * pos_im_relation\n",
        "  pos_im_score = pos_re_head * pos_im_relation + pos_im_head * pos_re_relation\n",
        "  pos_score = pos_re_score * pos_re_tail + pos_im_score * pos_im_tail\n",
        "  pos_loss = -F.logsigmoid(pos_score.sum(1))\n",
        "\n",
        "\n",
        "  # Compute neg score\n",
        "  neg_re_score = neg_re_head * neg_re_relation - neg_im_head * neg_im_relation\n",
        "  neg_im_score = neg_re_head * neg_im_relation + neg_im_head * neg_re_relation\n",
        "  neg_score = neg_re_score * neg_re_tail + neg_im_score * neg_im_tail\n",
        "  neg_loss = -F.logsigmoid(-neg_score.sum(1))\n",
        "\n",
        "  loss = pos_loss + neg_loss\n",
        "  reg_loss = reg * (\n",
        "      pos_re_head.norm(p=2, dim=1)**2 + pos_im_head.norm(p=2, dim=1)**2 +\n",
        "      pos_re_tail.norm(p=2, dim=1)**2 + pos_im_tail.norm(p=2, dim=1)**2 +\n",
        "      neg_re_head.norm(p=2, dim=1)**2 + neg_im_head.norm(p=2, dim=1)**2 +\n",
        "      neg_re_tail.norm(p=2, dim=1)**2 + neg_im_tail.norm(p=2, dim=1)**2 +\n",
        "      pos_re_relation.norm(p=2, dim=1)**2 + pos_im_relation.norm(p=2, dim=1)**2 +\n",
        "      neg_re_relation.norm(p=2, dim=1)**2 + neg_im_relation.norm(p=2, dim=1)**2)\n",
        "  loss += reg_loss\n",
        "  return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs6awu9l_zx0"
      },
      "source": [
        "**RotatE**\n",
        "\n",
        "---\n",
        "\n",
        "RotatE model can be seen as equivalent to TransE but in complex space. In this model, relations give angular rotation to the head entity embedding by an angle so as to make it closer to the tail entity embedding.\n",
        "\n",
        "The scoring function can be defined as - || h 𝗈 r - t || just like TransE but here we use rotation operator 'o' instead of simple addition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm1-mhca_2d6"
      },
      "outputs": [],
      "source": [
        "class RotatE(nn.Module):\n",
        "  def __init__(self, num_entities, num_relations, embedding_dim):\n",
        "    super(RotatE, self).__init__()\n",
        "    # entity embeddings has equal real and imaginary parts, so we double the dimension size\n",
        "    self.entity_embeddings = torch.nn.Parameter(torch.randn(num_entities, 2*embedding_dim))\n",
        "    self.relation_embeddings = torch.nn.Parameter(torch.randn(num_relations, embedding_dim))\n",
        "\n",
        "  def forward(self):\n",
        "    # return the embeddings as it is but we can regularize here by normalizing them\n",
        "    return self.entity_embeddings, self.relation_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRTHH64yTRhc"
      },
      "source": [
        "RotatE Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV0m5o2PCWnY"
      },
      "outputs": [],
      "source": [
        "def RotatE_loss(pos_edges, neg_edges, pos_reltype, neg_reltype, entity_embeddings, relation_embeddings,\n",
        "                gamma=5.0, epsilon=2.0):\n",
        "  # Select embeddings for both positive and negative samples\n",
        "  pos_head_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 0])\n",
        "  pos_tail_embeds = torch.index_select(entity_embeddings, 0, pos_edges[:, 1])\n",
        "  neg_head_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 0])\n",
        "  neg_tail_embeds = torch.index_select(entity_embeddings, 0, neg_edges[:, 1])\n",
        "  pos_relation_embeds = torch.index_select(relation_embeddings, 0, pos_reltype.squeeze())\n",
        "  neg_relation_embeds = torch.index_select(relation_embeddings, 0, neg_reltype.squeeze())\n",
        "\n",
        "  # Dissect the embedding in equal chunks to get real and imaginary parts\n",
        "  pos_re_head, pos_im_head = torch.chunk(pos_head_embeds, 2, dim=1)\n",
        "  pos_re_tail, pos_im_tail = torch.chunk(pos_tail_embeds, 2, dim=1)\n",
        "  neg_re_head, neg_im_head = torch.chunk(neg_head_embeds, 2, dim=1)\n",
        "  neg_re_tail, neg_im_tail = torch.chunk(neg_tail_embeds, 2, dim=1)\n",
        "\n",
        "  # Make phases of relations uniformly distributed in [-pi, pi]\n",
        "  embedding_range = 2 * (gamma + epsilon) / pos_head_embeds.size(-1)\n",
        "  pos_phase_relation = pos_relation_embeds/(embedding_range/np.pi)\n",
        "\n",
        "  pos_re_relation = torch.cos(pos_phase_relation)\n",
        "  pos_im_relation = torch.sin(pos_phase_relation)\n",
        "\n",
        "  neg_phase_relation = neg_relation_embeds/(embedding_range/np.pi)\n",
        "  neg_re_relation = torch.cos(neg_phase_relation)\n",
        "  neg_im_relation = torch.sin(neg_phase_relation)\n",
        "\n",
        "\n",
        "  # Compute pos score\n",
        "  pos_re_score = pos_re_head * pos_re_relation - pos_im_head * pos_im_relation\n",
        "  pos_im_score = pos_re_head * pos_im_relation + pos_im_head * pos_re_relation\n",
        "  pos_re_score = pos_re_score - pos_re_tail\n",
        "  pos_im_score = pos_im_score - pos_im_tail\n",
        "  # Stack and take squared norm of real and imaginary parts\n",
        "  pos_score = torch.stack([pos_re_score, pos_im_score], dim = 0)\n",
        "  pos_score = pos_score.norm(dim = 0)\n",
        "  # Log sigmoid of margin loss\n",
        "  pos_score = gamma - pos_score.sum(dim = 1)\n",
        "  pos_score = - F.logsigmoid(pos_score)\n",
        "\n",
        "  # Compute neg score\n",
        "  neg_re_score = neg_re_head * neg_re_relation - neg_im_head *neg_im_relation\n",
        "  neg_im_score = neg_re_head * neg_im_relation + neg_im_head * neg_re_relation\n",
        "  neg_re_score = neg_re_score - neg_re_tail\n",
        "  neg_im_score = neg_im_score - neg_im_tail\n",
        "  # Stack and take squared norm of real and imaginary parts\n",
        "  neg_score = torch.stack([neg_re_score, neg_im_score], dim = 0)\n",
        "  neg_score = neg_score.norm(dim = 0)\n",
        "  # Log sigmoid of margin loss\n",
        "  neg_score = gamma - neg_score.sum(dim = 1)\n",
        "  neg_score = - F.logsigmoid(-neg_score)\n",
        "\n",
        "  loss = (pos_score + neg_score)/2\n",
        "\n",
        "  return loss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBwpCeODTz8m"
      },
      "source": [
        "# Metrics and Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIVwT2Lmq4Jh"
      },
      "source": [
        "Helper routine to get the metric values given the predicted scores for a bunch of negative samples along with a positive sample that is always the first element at index 0. We currently have functionality to report these metrics:\n",
        "\n",
        "1) Hits@1\n",
        "\n",
        "2) Hits@3\n",
        "\n",
        "3) Hits@10\n",
        "\n",
        "4) Mean Rank\n",
        "\n",
        "5) Mean Reciprocal Rank"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8fCeSDuTM_X"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(y_pred):\n",
        "  argsort = torch.argsort(y_pred, dim = 1, descending = False)\n",
        "  # not using argsort to do the rankings to avoid bias when the scores are equal\n",
        "  ranking_list = torch.nonzero(argsort == 0, as_tuple=False)\n",
        "  ranking_list = ranking_list[:, 1] + 1\n",
        "  hits1_list = (ranking_list <= 1).to(torch.float)\n",
        "  hits3_list = (ranking_list <= 3).to(torch.float)\n",
        "  hits10_list = (ranking_list <= 10).to(torch.float)\n",
        "  mr_list = ranking_list.to(torch.float)\n",
        "  mrr_list = 1./ranking_list.to(torch.float)\n",
        "  return hits1_list.mean(), hits3_list.mean(), hits10_list.mean(), mr_list.mean(), mrr_list.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBRXsl7mraYq"
      },
      "source": [
        "Evaluation routine which given a head and relation, it ranks the original positive entity along with a bunch of negative entities on the basis of scoring criteria per model and calculates above metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mn8uXWDTzde"
      },
      "outputs": [],
      "source": [
        "def eval(entity_embeddings, relation_embeddings, dataloader, kg_model, iters=None, gamma = 5.0, epsilon = 2.0):\n",
        "\n",
        "  hits1_list = []\n",
        "  hits3_list = []\n",
        "  hits10_list = []\n",
        "  mr_list = []\n",
        "  mrr_list = []\n",
        "  data_iterator = iter(dataloader)\n",
        "  if iters is None:\n",
        "    iters = len(dataloader)\n",
        "  for _ in tqdm.trange(iters, desc=\"Evaluating\"):\n",
        "    batch = next(data_iterator)\n",
        "    edges, edge_reltype = batch\n",
        "    b, num_samples, _= edges.size()\n",
        "    edges = edges.view(b*num_samples, -1)\n",
        "    edge_reltype = edge_reltype.view(b*num_samples, -1)\n",
        "\n",
        "    head_embeds = torch.index_select(entity_embeddings, 0, edges[:, 0])\n",
        "    relation_embeds = torch.index_select(relation_embeddings, 0, edge_reltype.squeeze())\n",
        "    tail_embeds = torch.index_select(entity_embeddings, 0, edges[:, 1])\n",
        "\n",
        "    if kg_model == \"TransE\":\n",
        "      scores = torch.norm(head_embeds + relation_embeds - tail_embeds, p=1, dim=1)\n",
        "    elif kg_model == \"ComplEx\":\n",
        "      # Get real and imaginary parts\n",
        "      re_relation, im_relation = torch.chunk(relation_embeds, 2, dim=1)\n",
        "      re_head, im_head = torch.chunk(head_embeds, 2, dim=1)\n",
        "      re_tail, im_tail = torch.chunk(tail_embeds, 2, dim=1)\n",
        "\n",
        "      # Compute scores\n",
        "      re_score = re_head * re_relation - im_head * im_relation\n",
        "      im_score = re_head * im_relation + im_head * re_relation\n",
        "      scores = (re_score * re_tail + im_score * im_tail)\n",
        "      # Negate as we want to rank scores in ascending order, lower the better\n",
        "      scores = - scores.sum(dim=1)\n",
        "    elif kg_model == \"RotatE\":\n",
        "      # Get real and imaginary parts\n",
        "      re_head, im_head = torch.chunk(head_embeds, 2, dim=1)\n",
        "      re_tail, im_tail = torch.chunk(tail_embeds, 2, dim=1)\n",
        "\n",
        "      # Make phases of relations uniformly distributed in [-pi, pi]\n",
        "      embedding_range = 2 * (gamma + epsilon) / head_embeds.size(-1)\n",
        "      phase_relation = relation_embeds/(embedding_range/np.pi)\n",
        "      re_relation = torch.cos(phase_relation)\n",
        "      im_relation = torch.sin(phase_relation)\n",
        "\n",
        "      # Compute scores\n",
        "      re_score = re_head * re_relation - im_head * im_relation\n",
        "      im_score = re_head * im_relation + im_head * re_relation\n",
        "      re_score = re_score - re_tail\n",
        "      im_score = im_score - im_tail\n",
        "      scores = torch.stack([re_score, im_score], dim = 0)\n",
        "      scores = scores.norm(dim = 0)\n",
        "      scores = scores.sum(dim = 1)\n",
        "    else:\n",
        "      raise ValueError(f'Unsupported model {kg_model}')\n",
        "\n",
        "    scores = scores.view(b, num_samples)\n",
        "\n",
        "    hits1, hits3, hits10, mr, mrr = eval_metrics(scores)\n",
        "    hits1_list.append(hits1.item())\n",
        "    hits3_list.append(hits3.item())\n",
        "    hits10_list.append(hits10.item())\n",
        "    mr_list.append(mr.item())\n",
        "    mrr_list.append(mrr.item())\n",
        "\n",
        "  hits1 = sum(hits1_list)/len(hits1_list)\n",
        "  hits3 = sum(hits3_list)/len(hits1_list)\n",
        "  hits10 = sum(hits10_list)/len(hits1_list)\n",
        "  mr = sum(mr_list)/len(hits1_list)\n",
        "  mrr = sum(mrr_list)/len(hits1_list)\n",
        "\n",
        "  return hits1, hits3, hits10, mr, mrr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qt6ttoDKLqr"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7ok2fY4O9EP",
        "outputId": "e778fe61-66db-495d-ff66-7955cd1b6d99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-823d26697255>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mComplEx_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mkg_model\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RotatE\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRotatE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_entities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_relations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRotatE_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'RotatE' is not defined"
          ]
        }
      ],
      "source": [
        "#@title Choose your model and training parameters\n",
        "kg_model = \"TransE\" #@param [\"TransE\", \"ComplEx\", \"RotatE\"]\n",
        "epochs = 1000 #@param {type:\"slider\", min:10, max:1000, step:10}\n",
        "batch_size = 128 #@param {type:\"number\"}\n",
        "learning_rate = 1e-3 #@param {type:\"number\"}\n",
        "\n",
        "num_entities = 14541\n",
        "num_relations = 237\n",
        "\n",
        "if kg_model == \"TransE\":\n",
        "    model = TransE(num_entities, num_relations, 100)\n",
        "    model_loss = TransE_loss\n",
        "elif kg_model == \"ComplEx\":\n",
        "    model = ComplEx(num_entities, num_relations, 100)\n",
        "    model_loss = ComplEx_loss\n",
        "elif kg_model == \"RotatE\":\n",
        "    model = RotatE(num_entities, num_relations, 50)\n",
        "    model_loss = RotatE_loss\n",
        "else:\n",
        "    raise ValueError('Unsupported model %s' % kg_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCEL8ln-LCqq",
        "outputId": "cd40adc8-793d-4ce3-813f-834b0ad27ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size 272115\n",
            "Val dataset size 17535\n",
            "Test dataset size 20466\n"
          ]
        }
      ],
      "source": [
        "num_workers = os.cpu_count()\n",
        "\n",
        "train_dataset = RelationDataset(train_edge, true_edges, filter=True)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_dataset = RelationDataset(valid_edge, true_edges, filter=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "val_eval_dataset = TestRelationDataset(valid_edge, true_edges, filter=True, num_neg=100)\n",
        "val_eval_dataloader = DataLoader(val_eval_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "test_dataset = TestRelationDataset(test_edge, true_edges, filter=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "print(f'Train dataset size {len(train_dataset)}')\n",
        "print(f'Val dataset size {len(val_dataset)}')\n",
        "print(f'Test dataset size {len(test_dataset)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BWrk3HvJRrpE",
        "outputId": "ca97bae0-d39f-4a6d-e037-b6c36e3741b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.56484375 hits@3:0.73359375 hits@10:0.89296875 mr:4.8546875 mrr:0.6753929495811463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:31<00:00, 23.17it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 1 loss: 0.06732275679896771 val_loss: 0.13547529454213858\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:42<00:00, 20.78it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 2 loss: 0.06815271085497913 val_loss: 0.13382548137302816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:43<00:00, 20.61it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 128.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 3 loss: 0.06740859360105568 val_loss: 0.1322372573679381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:44<00:00, 20.42it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 4 loss: 0.06717213636276471 val_loss: 0.1303546155129906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:44<00:00, 20.28it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 5 loss: 0.06773391526686562 val_loss: 0.12991780880158835\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:45<00:00, 20.21it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 6 loss: 0.06737704628680746 val_loss: 0.1364327196276536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:44<00:00, 20.30it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 7 loss: 0.0678763163091154 val_loss: 0.14179258385713953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:45<00:00, 20.18it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 8 loss: 0.06769377247606485 val_loss: 0.13302352904838366\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:44<00:00, 20.27it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.11it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 9 loss: 0.06660719561414234 val_loss: 0.13577832140191629\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:45<00:00, 20.10it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 126.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 10 loss: 0.06728317480804162 val_loss: 0.1335805355414857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.578125 hits@3:0.76015625 hits@10:0.8984375 mr:4.7578125 mrr:0.689726036787033\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:46<00:00, 19.95it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 11 loss: 0.06523290240430989 val_loss: 0.13541864728840597\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:47<00:00, 19.81it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 12 loss: 0.06717195082543487 val_loss: 0.13880689714076747\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:46<00:00, 19.91it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 13 loss: 0.06490396440505757 val_loss: 0.13597170589831623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.48it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 14 loss: 0.0661570481948996 val_loss: 0.13254227200998878\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:47<00:00, 19.79it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 15 loss: 0.06525033334601296 val_loss: 0.13767806391646392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:48<00:00, 19.57it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 16 loss: 0.06459006619125408 val_loss: 0.13286802161784067\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:47<00:00, 19.74it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 17 loss: 0.06419271275730538 val_loss: 0.13324520829385214\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:48<00:00, 19.59it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 18 loss: 0.0642747001605054 val_loss: 0.1345919772003689\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.49it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 19 loss: 0.06488736244733335 val_loss: 0.13554537486638465\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:48<00:00, 19.63it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 20 loss: 0.06448578650328074 val_loss: 0.12409671216550534\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.60859375 hits@3:0.77265625 hits@10:0.90859375 mr:4.3953125 mrr:0.7107583105564117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.25it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 21 loss: 0.0640021462084573 val_loss: 0.12833160841769545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.44it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.20it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 22 loss: 0.06513504317049586 val_loss: 0.12338607274267795\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.32it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 23 loss: 0.06300868060324176 val_loss: 0.1283877404936909\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.32it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 24 loss: 0.06405497268052564 val_loss: 0.1365479872065739\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:48<00:00, 19.53it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 25 loss: 0.06385552423707405 val_loss: 0.12870054849742973\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.49it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 26 loss: 0.06388593864603527 val_loss: 0.12837177460646107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.40it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 27 loss: 0.06203029844787249 val_loss: 0.12539993998778126\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.50it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 117.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 28 loss: 0.06268597584522678 val_loss: 0.13803553477908573\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.44it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 29 loss: 0.062352961754585816 val_loss: 0.13201842625645827\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.28it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 30 loss: 0.06311829821824298 val_loss: 0.13281022632209055\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.83s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.61953125 hits@3:0.771875 hits@10:0.91796875 mr:3.64609375 mrr:0.7171742439270019\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.29it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 31 loss: 0.06339187417372094 val_loss: 0.13781524531162567\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.11it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 32 loss: 0.061302112194649025 val_loss: 0.1268102727340956\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.35it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 33 loss: 0.0635229965298667 val_loss: 0.12343359069667593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.26it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 34 loss: 0.06257683930608425 val_loss: 0.12605412641580957\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.34it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 35 loss: 0.061347077946727946 val_loss: 0.12660584283353638\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.27it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 36 loss: 0.0629254228077121 val_loss: 0.12309780281825658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.30it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 37 loss: 0.06209421394391096 val_loss: 0.12583666659184617\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.40it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 38 loss: 0.06254832747261814 val_loss: 0.12766732318993032\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.28it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 39 loss: 0.061947127022655195 val_loss: 0.1323207634208846\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.27it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 40 loss: 0.0628026095188292 val_loss: 0.12902316287921292\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.59296875 hits@3:0.75390625 hits@10:0.90546875 mr:4.50234375 mrr:0.6972998261451722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.19it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.06it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 41 loss: 0.06344252089187433 val_loss: 0.13427488418826222\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.97it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 42 loss: 0.06262283212910928 val_loss: 0.12375084560935515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.18it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.90it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 43 loss: 0.0613630084024772 val_loss: 0.1337016699087881\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.06it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 44 loss: 0.061028287838005506 val_loss: 0.12985379420166468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.21it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 45 loss: 0.06305165867113552 val_loss: 0.1265566684033749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.02it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 46 loss: 0.062352607112556614 val_loss: 0.12791437421836993\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.15it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 47 loss: 0.06261982849990969 val_loss: 0.13100031175970162\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 18.98it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 48 loss: 0.061175996349618074 val_loss: 0.12804586249981484\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.18it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 126.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 49 loss: 0.061367327538393406 val_loss: 0.13037993601203834\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.04it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 121.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 50 loss: 0.06127775424034777 val_loss: 0.13209140703190853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.5703125 hits@3:0.7515625 hits@10:0.90546875 mr:4.7765625 mrr:0.6823648869991302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.16it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 51 loss: 0.06139210858036215 val_loss: 0.12774795195917144\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:53<00:00, 18.80it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 52 loss: 0.06162340679010578 val_loss: 0.125828335156841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.06it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 118.10it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 53 loss: 0.06079078029914779 val_loss: 0.1271802927673298\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.89it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 54 loss: 0.06068019395096618 val_loss: 0.12673136091580356\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.16it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 119.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 55 loss: 0.0616458673126512 val_loss: 0.1293240441893139\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.84it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 56 loss: 0.06100849273175646 val_loss: 0.12268437124299307\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.14it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 57 loss: 0.06108302475318579 val_loss: 0.126083584053673\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.82it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 58 loss: 0.06044336501859402 val_loss: 0.12630175891583853\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.13it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 128.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 59 loss: 0.061611378688088365 val_loss: 0.12025651933938047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:53<00:00, 18.76it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 126.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 60 loss: 0.05981244577527719 val_loss: 0.12606344170814013\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.57578125 hits@3:0.76640625 hits@10:0.91171875 mr:4.30390625 mrr:0.690211683511734\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.33it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 128.93it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 61 loss: 0.06108698402711051 val_loss: 0.12236687666090736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.20it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 62 loss: 0.05987752282594244 val_loss: 0.12562453670658336\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.46it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 63 loss: 0.06131961420949405 val_loss: 0.122855026410879\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.24it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 64 loss: 0.06105735360076152 val_loss: 0.12874791075060837\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.39it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 127.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 65 loss: 0.060548695110955106 val_loss: 0.12329026023401832\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.24it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 66 loss: 0.05904878550476324 val_loss: 0.13020281276128587\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:49<00:00, 19.39it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 126.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 67 loss: 0.06044156458281394 val_loss: 0.12808186620691397\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.29it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 68 loss: 0.060264150744351375 val_loss: 0.12489436371048002\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.30it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 69 loss: 0.060929585388412144 val_loss: 0.11950301580185438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.10it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 70 loss: 0.059773669778768 val_loss: 0.12354948328141749\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating: 100%|██████████| 10/10 [00:17<00:00,  1.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hits@1:0.60625 hits@3:0.7703125 hits@10:0.9015625 mr:4.284375 mrr:0.7102800905704498\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:50<00:00, 19.17it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 71 loss: 0.05952392439318511 val_loss: 0.12237205535826022\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 18.98it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 117.85it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 72 loss: 0.06008168713996574 val_loss: 0.12194965268573622\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.93it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 73 loss: 0.06006213090788163 val_loss: 0.1242610168283003\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.84it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 120.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 74 loss: 0.05935092703881739 val_loss: 0.12941154478675257\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.00it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 124.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 75 loss: 0.059488600086655574 val_loss: 0.12631836446532368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.01it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 122.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 76 loss: 0.05968007248907805 val_loss: 0.12246432356590772\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:52<00:00, 18.88it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 125.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 77 loss: 0.05928861279380512 val_loss: 0.12420323306191576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.02it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 127.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 78 loss: 0.05964645768252051 val_loss: 0.12497306680374771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 2126/2126 [01:51<00:00, 19.09it/s]\n",
            "Validating: 100%|██████████| 137/137 [00:01<00:00, 123.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 79 loss: 0.058969546395055215 val_loss: 0.1220502016970711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  77%|███████▋  | 1641/2126 [01:25<00:24, 19.63it/s]"
          ]
        }
      ],
      "source": [
        "# use adam optimizer for training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for e in range(epochs):\n",
        "  losses = []\n",
        "  # check evaluation metrics every 10th epoch\n",
        "  if e%10 == 0:\n",
        "    model.eval()\n",
        "    h1, h3, h10, mr, mrr = eval(model.entity_embeddings, model.relation_embeddings, val_eval_dataloader, kg_model, iters=10)\n",
        "    print(f\"hits@1:{h1} hits@3:{h3} hits@10:{h10} mr:{mr} mrr:{mrr}\")\n",
        "  model.train()\n",
        "  for step, batch in enumerate(tqdm.tqdm(train_dataloader, desc=\"Training\")):\n",
        "    # generate positive as well as negative samples for training\n",
        "    pos_sample, neg_sample = batch\n",
        "    # do a forward pass through the model\n",
        "    entity_embeddings_pass, relation_embeddings_pass = model()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # compute the loss as per your model scoring criteria\n",
        "    loss = model_loss(pos_sample[0], neg_sample[0], pos_sample[1], neg_sample[1],\n",
        "                      entity_embeddings_pass, relation_embeddings_pass)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "  val_losses = []\n",
        "  model.eval()\n",
        "  entity_embeddings_pass, relation_embeddings_pass = model()\n",
        "  # compute validation loss on unseen samples we didn't train on\n",
        "  for step, batch in enumerate(tqdm.tqdm(val_dataloader, desc=\"Validating\")):\n",
        "    pos_sample, neg_sample = batch\n",
        "    loss = model_loss(pos_sample[0], neg_sample[0], pos_sample[1], neg_sample[1],\n",
        "                      entity_embeddings_pass, relation_embeddings_pass)\n",
        "    val_losses.append(loss.item())\n",
        "\n",
        "  print(f\"epoch: {e + 1} loss: {sum(losses)/len(losses)} val_loss: {sum(val_losses)/len(val_losses)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRCM0pFyiRWk"
      },
      "source": [
        "Now let's test if our model actually learned something!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7jShxzDu-Eu",
        "outputId": "581a8066-8cfd-400c-a421-83b3ca48dec1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 1000/1000 [08:51<00:00,  1.88it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.119, 0.215, 0.369, 193.727, 0.20023192654850572)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Test your trained model\n",
        "iterations = 1000 #@param {type:\"slider\", min:100, max:2000, step:100}\n",
        "mode = \"head\" #@param [\"head\", \"tail\"]\n",
        "\n",
        "model.eval()\n",
        "test_dataset = TestRelationDataset(test_edge, true_edges, filter=True, mode=mode)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "eval(model.entity_embeddings, model.relation_embeddings, test_dataloader, kg_model, iters=iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAMIIWHNNrBP"
      },
      "source": [
        "For reference, we tabulate the results we obtained for different models here:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}